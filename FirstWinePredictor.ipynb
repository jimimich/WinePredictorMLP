{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jimi Michael, B455 Project 1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import KFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',          \n",
    "    names = ( 'Class', 'Alcohol', 'Malic Acid', 'Ash', 'Alcalinity of Ash', 'Magnesium', 'Total Phenols', 'Flavanoids', \n",
    "             'Nonflavanoid Phenols', 'Proanthocyanins', 'Color Intensity', 'Hue', 'OD280/OD315 of Diluted Wines', 'Proline'))\n",
    "\n",
    "addition = pd.get_dummies(data['Class'])\n",
    "data = pd.concat([data,addition],axis= 1)\n",
    "x = data.drop([1, 2, 3,'Class'], axis = 1)\n",
    "y = data[[1, 2, 3]].values\n",
    "\n",
    "#softmax activation\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "#loss\n",
    "def loss_derivative(y, yPred):\n",
    "    return (yPred - y)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP initialization \n",
    "def myModel(input, hidden, output):\n",
    "    # in\n",
    "    w1 = 2 *np.random.randn(input, hidden) - 1\n",
    "    b1 = np.zeros((1, hidden))\n",
    "    # hid\n",
    "    w2 = 2 * np.random.randn(hidden, hidden) - 1\n",
    "    b2 = np.zeros((1, hidden))\n",
    "    #out\n",
    "    w3 = 2 * np.random.rand(hidden, output) - 1\n",
    "    b3 = np.zeros((1,output))\n",
    "    \n",
    "\n",
    "    return {'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2, 'w3': w3,'b3': b3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to handle forward propogation\n",
    "def forwardProp(model, activ):\n",
    "  \n",
    "  # Get mapped model data\n",
    "  w1 = model['w1'] \n",
    "  b1 = model['b1'] \n",
    "  w2 = model['w2'] \n",
    "  b2 = model['b2'] \n",
    "  w3 = model['w3'] \n",
    "  b3 = model['b3'] \n",
    "  \n",
    "  # First step tanh\n",
    "  firstLine = activ.dot(w1) + b1\n",
    "  firstAct = np.tanh(firstLine)\n",
    "  \n",
    "  # Second step softmax\n",
    "  secLine = firstAct.dot(w2) + b2\n",
    "  secAct = softmax(secLine)\n",
    "  \n",
    "  # Third step softmax\n",
    "  thirdLine = secAct.dot(w3) + b3\n",
    "  thirdAct = softmax(thirdLine)\n",
    "  \n",
    "  # Return map of forwardProp\n",
    "  return {'activ': activ, 'linOne': firstLine, 'firstAct': firstAct, 'secLine': secLine, 'secAct': secAct,\n",
    "                        'thirdLine': thirdLine, 'thirdAct' : thirdAct}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPropogation(model, forProp, y):\n",
    "    #Data from model\n",
    "    w1 = model['w1'] \n",
    "    b1 = model['b1'] \n",
    "    w2 = model['w2'] \n",
    "    b2 = model['b2'] \n",
    "    w3 = model['w3'] \n",
    "    b3 = model['b3'] \n",
    "    \n",
    "    # Data from forProp\n",
    "    activ = forProp['activ']\n",
    "    firstAct = forProp['firstAct']\n",
    "    secAct = forProp['secAct']\n",
    "    thirdAct = forProp['thirdAct']\n",
    "    m = y.shape[0]\n",
    "\n",
    "    # Loss calulations, goes backwords\n",
    "    thirdLine = loss_derivative(y, thirdAct)\n",
    "    lossWeight3 = (secAct.T).dot(thirdLine) * (1/m)\n",
    "    lossBias3 = np.sum(thirdLine, axis=0) * (1/m)\n",
    "    \n",
    "    secLine = np.multiply(thirdLine.dot(w3.T), tanh_derivative(secAct))\n",
    "    lossWeight2 = np.dot(firstAct.T, secLine)* (1/m)\n",
    "    lossBias2 = np.sum(secLine, axis=0)* (1/m)\n",
    "    \n",
    "    firstLine = np.multiply(secLine.dot(w2.T), tanh_derivative(firstAct))\n",
    "    lossWeight1 = np.dot(activ.T, firstLine)* (1/m)\n",
    "    lossBias1 = np.sum(firstLine, axis=0)* (1/m)\n",
    "    \n",
    "    #Weights after backProp\n",
    "    return {'lw3': lossWeight3, 'lb3': lossBias3, 'lw2': lossWeight2, 'lb2': lossBias2, 'lw1': lossWeight1,'lb1': lossBias1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateWeightBias(model, backProp, learnRate):\n",
    "    # Load original weights and biases\n",
    "    weightOne, biasOne, weightTwo, biasTwo, weightThree, biasThree = model['w1'], model['b1'], model['w2'], model['b2'], model['w3'], model['b3']\n",
    "    \n",
    "    # Update weights and biases\n",
    "    weightOne -= backProp['lw1'] * learnRate\n",
    "    biasOne -=  backProp['lb1'] * learnRate\n",
    "    weightTwo -= backProp['lw2'] * learnRate\n",
    "    biasTwo -= backProp['lb2'] * learnRate\n",
    "    weightThree -= backProp['lw3'] * learnRate\n",
    "    biasThree -= backProp['lb3'] * learnRate\n",
    "    \n",
    "    # Return updated values\n",
    "   \n",
    "    return {'w1': weightOne, 'b1': biasOne, 'w2': weightTwo, 'b2': biasTwo, 'w3': weightThree,'b3': biasThree}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy calculation\n",
    "def accuracy(testing, actual):\n",
    "  incorrect = 0\n",
    "  for i in range(len(testing)):\n",
    "    if((testing[i] != actual[i]).any()):\n",
    "      incorrect = incorrect + 1\n",
    "  return 1 - (incorrect/len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model prediction\n",
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    fp = forwardProp(model, x)\n",
    "    yPred = np.argmax(fp['thirdAct'], axis=1)\n",
    "    return yPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training\n",
    "def train(model, x , y, learnRate, count):\n",
    "    \n",
    "    for i in range(0, count):\n",
    "        fp = forwardProp(model, x) # Forward propagation\n",
    "        bp = backwardPropogation(model, fp, y) # Back propagation\n",
    "        model = updateWeightBias(model, bp, learnRate) # Update weights and bias\n",
    "    yPred = predict(model, x)\n",
    "    yActual = y.argmax(axis=1)\n",
    "    \n",
    "    print('Accuracy: ', accuracy(yPred, yActual))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets random guess\n",
    "def randomGuess(real):\n",
    "  output = []\n",
    "  \n",
    "  for i in range(0,len(real)):\n",
    "    \n",
    "    x = np.random.randint(3)+1\n",
    "    if x == 1:\n",
    "      output.append([1, 0, 0])\n",
    "    elif x == 2:\n",
    "      output.append([0, 1, 0])\n",
    "    else:\n",
    "      output.append([0, 0, 1])\n",
    "  output = np.array(output)\n",
    "  \n",
    "  print('Random Guess Accuracy:', accuracy(output, real) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [  0   1   4   5   6   7   8   9  10  11  12  14  15  16  18  19  20  21\n",
      "  22  24  26  27  29  30  32  33  34  35  36  37  38  39  40  42  43  44\n",
      "  45  46  47  48  49  50  51  52  53  54  55  56  57  60  61  62  63  65\n",
      "  66  67  68  69  70  71  72  74  75  76  77  78  80  83  85  86  87  88\n",
      "  89  92  93  94  95  96  97  98 100 101 102 103 104 105 106 107 109 110\n",
      " 112 114 115 117 118 119 120 121 122 123 124 125 126 127 133 134 136 137\n",
      " 138 140 142 143 144 145 146 147 149 150 151 152 153 154 155 156 158 159\n",
      " 160 161 162 163 164 165 166 167 168 169 171 172 174 175 176 177] TEST: [  2   3  13  17  23  25  28  31  41  58  59  64  73  79  81  82  84  90\n",
      "  91  99 108 111 113 116 128 129 130 131 132 135 139 141 148 157 170 173]\n",
      "Accuracy:  1.0\n",
      "Random Guess Accuracy: 0.38888888888888884\n",
      "TRAIN: [  0   1   2   3   5   7  10  13  14  15  16  17  20  21  22  23  25  26\n",
      "  27  28  29  31  32  33  34  35  37  38  39  41  42  43  44  47  48  49\n",
      "  50  51  52  54  55  57  58  59  61  62  63  64  65  66  67  68  69  71\n",
      "  72  73  74  76  77  78  79  80  81  82  84  85  86  87  88  89  90  91\n",
      "  92  93  94  97  99 100 101 102 103 104 105 106 107 108 109 110 111 112\n",
      " 113 116 117 118 119 122 125 126 127 128 129 130 131 132 133 134 135 136\n",
      " 137 138 139 140 141 143 144 145 148 149 150 151 152 153 154 155 156 157\n",
      " 158 159 161 163 166 167 168 169 170 171 172 173 174 175 176 177] TEST: [  4   6   8   9  11  12  18  19  24  30  36  40  45  46  53  56  60  70\n",
      "  75  83  95  96  98 114 115 120 121 123 124 142 146 147 160 162 164 165]\n",
      "Accuracy:  1.0\n",
      "Random Guess Accuracy: 0.3055555555555556\n",
      "TRAIN: [  1   2   3   4   5   6   8   9  11  12  13  15  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  34  35  36  37  38  39  40  41\n",
      "  42  43  44  45  46  47  49  50  51  53  54  55  56  58  59  60  62  64\n",
      "  66  68  70  71  72  73  74  75  76  77  78  79  81  82  83  84  85  87\n",
      "  88  89  90  91  92  93  94  95  96  97  98  99 100 103 104 106 108 109\n",
      " 111 112 113 114 115 116 118 119 120 121 122 123 124 127 128 129 130 131\n",
      " 132 133 135 138 139 140 141 142 143 146 147 148 152 153 154 156 157 158\n",
      " 159 160 161 162 163 164 165 166 167 168 169 170 173 174 176 177] TEST: [  0   7  10  14  16  33  48  52  57  61  63  65  67  69  80  86 101 102\n",
      " 105 107 110 117 125 126 134 136 137 144 145 149 150 151 155 171 172 175]\n",
      "Accuracy:  1.0\n",
      "Random Guess Accuracy: 0.3055555555555556\n",
      "TRAIN: [  0   2   3   4   6   7   8   9  10  11  12  13  14  16  17  18  19  21\n",
      "  23  24  25  26  28  29  30  31  33  34  35  36  38  39  40  41  42  43\n",
      "  44  45  46  48  49  50  51  52  53  55  56  57  58  59  60  61  62  63\n",
      "  64  65  66  67  69  70  71  73  74  75  76  77  78  79  80  81  82  83\n",
      "  84  85  86  87  90  91  93  94  95  96  98  99 101 102 104 105 107 108\n",
      " 110 111 113 114 115 116 117 118 120 121 122 123 124 125 126 128 129 130\n",
      " 131 132 133 134 135 136 137 138 139 141 142 144 145 146 147 148 149 150\n",
      " 151 152 155 156 157 158 159 160 162 164 165 170 171 172 173 175 176] TEST: [  1   5  15  20  22  27  32  37  47  54  68  72  88  89  92  97 100 103\n",
      " 106 109 112 119 127 140 143 153 154 161 163 166 167 168 169 174 177]\n",
      "Accuracy:  1.0\n",
      "Random Guess Accuracy: 0.4\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  22  23  24  25  27  28  30  31  32  33  36  37  40  41  45\n",
      "  46  47  48  52  53  54  56  57  58  59  60  61  63  64  65  67  68  69\n",
      "  70  72  73  75  79  80  81  82  83  84  86  88  89  90  91  92  95  96\n",
      "  97  98  99 100 101 102 103 105 106 107 108 109 110 111 112 113 114 115\n",
      " 116 117 119 120 121 123 124 125 126 127 128 129 130 131 132 134 135 136\n",
      " 137 139 140 141 142 143 144 145 146 147 148 149 150 151 153 154 155 157\n",
      " 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 177] TEST: [ 21  26  29  34  35  38  39  42  43  44  49  50  51  55  62  66  71  74\n",
      "  76  77  78  85  87  93  94 104 118 122 133 138 152 156 158 159 176]\n",
      "Accuracy:  1.0\n",
      "Random Guess Accuracy: 0.2571428571428571\n"
     ]
    }
   ],
   "source": [
    "# Initialization of KFold Algorithm\n",
    "kfold = KFold(n_splits = 5, random_state = None, shuffle = True)\n",
    "\n",
    "for trainIndex, testIndex in kfold.split(data):\n",
    "  print(\"TRAIN:\", trainIndex, \"TEST:\", testIndex) # Prints which data rows are in the training and testing sets\n",
    "  XTrain, XTest = x.iloc[trainIndex], x.iloc[testIndex] \n",
    "  YTrain, YTest = y[trainIndex], y[testIndex]\n",
    "  \n",
    "  # Batch normalizes the data using a standard scalar \n",
    "  scaler = StandardScaler()\n",
    "  scaler.fit(XTrain)\n",
    "  XTrain = scaler.transform(XTrain)\n",
    "  XTest = scaler.transform(XTest)\n",
    "  \n",
    "  # Implementation of Model\n",
    "  model = myModel(13, 5, 3) # Input layer = 13; Hidden layer = 7; Output layer = 3 (Tested out various hidden layer values)\n",
    "  model = train(model, XTrain, YTrain, 0.5, 5000)\n",
    "  randomGuess(YTest)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
